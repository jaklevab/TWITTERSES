{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reminder: Not running filters based on users.db\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "uk = '+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 \\\n",
    "+x_0=400000 +y_0=-100000 +ellps=airy \\\n",
    "+towgs84=446.448,-125.157,542.06,0.15,0.247,0.842,-20.489 +units=m +no_defs'\n",
    "\n",
    "from tqdm import tqdm as tqdmn\n",
    "import pandas as pd\n",
    "from geopandas import GeoDataFrame\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from geopandas import sjoin\n",
    "import pickle\n",
    "import re;pat = re.compile(r'''(-*\\d+\\.\\d+ -*\\d+\\.\\d+);*''');new_geo=[]\n",
    "import warnings\n",
    "import sys\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sys.path.append('/warehouse/COMPLEXNET/jlevyabi/ml_soc_econ/data_files/UKSOC_rep/')\n",
    "from IPython.display import clear_output\n",
    "from pyproj import transform,Proj\n",
    "import numpy as np\n",
    "sys.path.append('/warehouse/COMPLEXNET/jlevyabi/network_representation/python_scripts/')\n",
    "import helpers_ses_prediction as hsp\n",
    "from helpers_locs_to_home import time_2_date\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from scipy.spatial import cKDTree\n",
    "from collections import Counter\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Geolocated Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Geolocated Profiles\n",
    "fgeo=\"/warehouse/COMPLEXNET/jlevyabi/ml_soc_econ/data_files/UKSOC_rep/\"\n",
    "prof_geolocated=pickle.load(open(fgeo+\"all_together_profiles.p\",\"rb\"))\n",
    "data_usr_description=pd.DataFrame([(usr[0].id,usr[0].description) for usr in prof_geolocated],\n",
    "                                 columns=[\"usr_id\",\"desc\"])\n",
    "# Geolocations\n",
    "dgeo=pd.read_csv(header=None,sep=\"\\t\",\n",
    "                 filepath_or_buffer=\"/warehouse/COMPLEXNET/jlevyabi/ml_soc_econ/data_files/2014-2015_locs.txt\")\n",
    "dgeo.columns=[\"usr\",\"time\",\"lat\",\"lon\",\"text\"]\n",
    "fechas,days,hours,minutes,seconds,years,months=time_2_date(dgeo.time)\n",
    "dgeo['day']=days;dgeo['hour']=hours;dgeo['min']=minutes;dgeo['sec']=seconds\n",
    "dgeo['year']=years;dgeo['month']=months;dgeo['fecha']=fechas\n",
    "france=Polygon([[-4.9658203125,42.3585439175],[8.4375,42.3585439175],\n",
    "                [8.4375,51.2344073516],[-4.9658203125,51.2344073516],\n",
    "                [-4.9658203125,42.3585439175]])\n",
    "locs=[Point((lon,lat)) for lon,lat in zip(dgeo.lon,dgeo.lat)]\n",
    "dgeo_france=dgeo[[france.contains(geo_pt) for geo_pt in locs]]\n",
    "\n",
    "dec_prec=5\n",
    "nb_rep_thresh=5\n",
    "pre_fake_locs=pd.read_csv(\"/warehouse/COMPLEXNET/jlevyabi/geoloc/txt_files/fake_locs.txt\",\n",
    "                          names=[\"pre\"])\n",
    "fake_locs=[]\n",
    "for x in pre_fake_locs.pre:\n",
    "    try:\n",
    "        lat,lon=float(x.split(\"-\")[0]),float(x.split(\"-\")[-1])\n",
    "    except:\n",
    "        continue\n",
    "    if len(x)==3:\n",
    "        fake_locs.append(str((lat,-lon)))\n",
    "    else:\n",
    "        fake_locs.append(str((lat,lon)))\n",
    "\n",
    "fake_locs=list(set(fake_locs))\n",
    "pre_fake=Counter([(round(x[0],dec_prec),round(x[1],dec_prec)) for x in  dgeo_france[[\"lat\",\"lon\"]].values])\n",
    "set_toher_fakes=set([x for x in pre_fake if pre_fake[x]>nb_rep_thresh])\n",
    "dic_final_not_nan=pickle.load(  open(\"/warehouse/COMPLEXNET/jlevyabi/ml_soc_econ/data_files/UKSOC_rep/all_together_dic.p\",\"rb\"))\n",
    "\n",
    "\n",
    "home_most_freq_all=go_through_home_candidates(dic_final_not_nan,take_most_frequent)\n",
    "home_most_freq_night=go_through_home_candidates(dic_final_not_nan,take_most_frequent_night)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different locations ... 1078482\n",
      "Number of repeated (=fake) locations ... 20579\n",
      "Number of geolocated users ... 22520\n",
      "Number of users not geolocated to fake locations... 19839\n"
     ]
    }
   ],
   "source": [
    "dic_all_users_inf_no_fake_insee={k:v for k,v in  home_most_freq_all.items()\n",
    "                           if not((round(v[\"lat\"],dec_prec),\n",
    "                                   round(v[\"lon\"],dec_prec))\n",
    "                                  in set_toher_fakes) }\n",
    "\n",
    "print(\"Number of different locations ... %d\"%len(pre_fake))\n",
    "print(\"Number of repeated (=fake) locations ... %d\"%len(set_toher_fakes))\n",
    "print(\"Number of geolocated users ... %d\"%len(home_most_freq_all))\n",
    "print(\"Number of users not geolocated to fake locations... %d\"%len(dic_all_users_inf_no_fake_insee))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "geol_home_df=pd.DataFrame([(v.usr,v.lat,v.lon,v.income,v.owner_ratio)\n",
    "                           for k,v in dic_all_users_inf_no_fake_insee.items()],\n",
    "                         columns=[\"usr\",\"lat\",\"lon\",\"income\",\"or\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LinkedIn Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>usr</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>income</th>\n",
       "      <th>or</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>878291414</td>\n",
       "      <td>47.116211</td>\n",
       "      <td>2.360299</td>\n",
       "      <td>27799.9375</td>\n",
       "      <td>0.3125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         usr        lat       lon      income      or\n",
       "0  878291414  47.116211  2.360299  27799.9375  0.3125"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geol_home_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import pickle,tweepy\n",
    "import re,numpy as np\n",
    "\n",
    "# LinkedIn users\n",
    "label_data=pd.read_csv(error_bad_lines=False,\n",
    "                       filepath_or_buffer=\"/warehouse/COMPLEXNET/jlevyabi/ml_soc_econ/data_files/UKSOC_rep/linkedin/linkedin_data/full_linkedin_sofar.csv\")\n",
    "\n",
    "regex_sals = re.compile('[^0-9]')\n",
    "treat_sal=lambda x: regex_sals.sub(\"\",x) if type(x) is str else \"\"\n",
    "salaries_per_usr=[]\n",
    "for it,row in  label_data[[\"Salary 1\",\"Salary 2\",\"Salary 3\",\"Salary 4\"]].iterrows():\n",
    "    sal1,sal2,sal3,sal4=map(treat_sal,row.values.tolist())\n",
    "    all_sals=[]\n",
    "    if type(sal1) is str and len(sal1)>0:all_sals.append(int(sal1))\n",
    "    if type(sal2) is str and len(sal2)>0:all_sals.append(int(sal2))\n",
    "    if type(sal3) is str and len(sal3)>0:all_sals.append(int(sal3))\n",
    "    if type(sal4) is str and len(sal4)>0:all_sals.append(int(sal4))\n",
    "    if len(all_sals)>0:\n",
    "        salaries_per_usr.append(np.min(all_sals))\n",
    "    else:\n",
    "        salaries_per_usr.append(np.nan)\n",
    "salaries_per_usr=np.array(salaries_per_usr)\n",
    "label_data[\"estimated_sal\"]=salaries_per_usr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>twitter_id</th>\n",
       "      <th>Title</th>\n",
       "      <th>income</th>\n",
       "      <th>estimated_sal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>399984229</td>\n",
       "      <td>Étudiant au centre polymétier</td>\n",
       "      <td>20642.421622</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12073732</td>\n",
       "      <td>Chargé de la veille et de la prospective à la ...</td>\n",
       "      <td>17376.389610</td>\n",
       "      <td>33585.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1152230846</td>\n",
       "      <td>Chargée de communication - Europe 1</td>\n",
       "      <td>21237.163876</td>\n",
       "      <td>50569.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>454343377</td>\n",
       "      <td>Data Analyst intern at Canal TP</td>\n",
       "      <td>17518.491429</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>465832857</td>\n",
       "      <td>Chef de publicité chez LM Work and Roll®</td>\n",
       "      <td>21981.489540</td>\n",
       "      <td>30754.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>473733086</td>\n",
       "      <td>Senior Director Digital Communication at Volvo...</td>\n",
       "      <td>24206.285086</td>\n",
       "      <td>30754.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>47969383</td>\n",
       "      <td>Data Marketing | Social Selling Practitioner |...</td>\n",
       "      <td>24561.303020</td>\n",
       "      <td>50569.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>127854864</td>\n",
       "      <td>Data Marketing | Social Selling Practitioner |...</td>\n",
       "      <td>24561.303020</td>\n",
       "      <td>50569.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>48657558</td>\n",
       "      <td>Chef de projet Marketing Digital &amp; e-Commerce ...</td>\n",
       "      <td>18614.403198</td>\n",
       "      <td>30754.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5707942</td>\n",
       "      <td>Head of Reputation &amp; Social Media Management e...</td>\n",
       "      <td>23829.452609</td>\n",
       "      <td>30754.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>579264682</td>\n",
       "      <td>Présentateur et rédacteur en chef adjoint de C...</td>\n",
       "      <td>18703.225000</td>\n",
       "      <td>51646.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>620063415</td>\n",
       "      <td>Chargée de communication chez Banque Populaire...</td>\n",
       "      <td>19300.202454</td>\n",
       "      <td>30754.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6313862</td>\n",
       "      <td>[NOT AVAILABLE / NOT OPEN FOR NEW OPPORTUNITY]...</td>\n",
       "      <td>20190.729323</td>\n",
       "      <td>32831.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>824990534</td>\n",
       "      <td>Gérant chez Kanble 🔶 Blockchain Evangelist 🔶 k...</td>\n",
       "      <td>20333.790123</td>\n",
       "      <td>32708.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>861557689</td>\n",
       "      <td>Service Départemental d'Incendie et de secours...</td>\n",
       "      <td>21995.431227</td>\n",
       "      <td>30646.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>87743325</td>\n",
       "      <td>Content strategist, directrice éditoriale, con...</td>\n",
       "      <td>24389.296296</td>\n",
       "      <td>30754.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>14094405</td>\n",
       "      <td>Consultant digital / social senior chez Double 2</td>\n",
       "      <td>19111.204754</td>\n",
       "      <td>30754.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>107066339</td>\n",
       "      <td>Recherche un contrat de professionnalisation, ...</td>\n",
       "      <td>13888.813433</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>231777033</td>\n",
       "      <td>Customer Success Manager chez Work4</td>\n",
       "      <td>16724.535104</td>\n",
       "      <td>50569.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24245549</td>\n",
       "      <td>Consultant social media, social branding, soci...</td>\n",
       "      <td>15761.636872</td>\n",
       "      <td>30754.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>26033900</td>\n",
       "      <td>Digital Content Manager Group Canal+</td>\n",
       "      <td>23982.516358</td>\n",
       "      <td>26969.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>28964962</td>\n",
       "      <td>Directeur des Clients Nationaux chez Adrexo - ...</td>\n",
       "      <td>26042.914286</td>\n",
       "      <td>57754.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1148500148</td>\n",
       "      <td>Collaborateur parlementaire chez Assemblée nat...</td>\n",
       "      <td>24663.246534</td>\n",
       "      <td>32908.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29683129</td>\n",
       "      <td>Head of Digital</td>\n",
       "      <td>21655.930452</td>\n",
       "      <td>30754.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>305168829</td>\n",
       "      <td>Graphiste Freelance</td>\n",
       "      <td>22484.937792</td>\n",
       "      <td>23862.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>352277053</td>\n",
       "      <td>Assistant à la direction des sports chez CANAL+</td>\n",
       "      <td>24075.615848</td>\n",
       "      <td>30754.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>387555825</td>\n",
       "      <td>Parle de Digital, de Social Media, de Start-Up...</td>\n",
       "      <td>16180.000000</td>\n",
       "      <td>27831.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    twitter_id                                              Title  \\\n",
       "0    399984229                      Étudiant au centre polymétier   \n",
       "1     12073732  Chargé de la veille et de la prospective à la ...   \n",
       "2   1152230846                Chargée de communication - Europe 1   \n",
       "6    454343377                    Data Analyst intern at Canal TP   \n",
       "7    465832857           Chef de publicité chez LM Work and Roll®   \n",
       "8    473733086  Senior Director Digital Communication at Volvo...   \n",
       "9     47969383  Data Marketing | Social Selling Practitioner |...   \n",
       "10   127854864  Data Marketing | Social Selling Practitioner |...   \n",
       "11    48657558  Chef de projet Marketing Digital & e-Commerce ...   \n",
       "12     5707942  Head of Reputation & Social Media Management e...   \n",
       "13   579264682  Présentateur et rédacteur en chef adjoint de C...   \n",
       "14   620063415  Chargée de communication chez Banque Populaire...   \n",
       "16     6313862  [NOT AVAILABLE / NOT OPEN FOR NEW OPPORTUNITY]...   \n",
       "17   824990534  Gérant chez Kanble 🔶 Blockchain Evangelist 🔶 k...   \n",
       "18   861557689  Service Départemental d'Incendie et de secours...   \n",
       "19    87743325  Content strategist, directrice éditoriale, con...   \n",
       "20    14094405   Consultant digital / social senior chez Double 2   \n",
       "21   107066339  Recherche un contrat de professionnalisation, ...   \n",
       "22   231777033                Customer Success Manager chez Work4   \n",
       "23    24245549  Consultant social media, social branding, soci...   \n",
       "24    26033900               Digital Content Manager Group Canal+   \n",
       "25    28964962  Directeur des Clients Nationaux chez Adrexo - ...   \n",
       "26  1148500148  Collaborateur parlementaire chez Assemblée nat...   \n",
       "27    29683129                                    Head of Digital   \n",
       "28   305168829                                Graphiste Freelance   \n",
       "29   352277053    Assistant à la direction des sports chez CANAL+   \n",
       "30   387555825  Parle de Digital, de Social Media, de Start-Up...   \n",
       "\n",
       "          income  estimated_sal  \n",
       "0   20642.421622            NaN  \n",
       "1   17376.389610        33585.0  \n",
       "2   21237.163876        50569.0  \n",
       "6   17518.491429            NaN  \n",
       "7   21981.489540        30754.0  \n",
       "8   24206.285086        30754.0  \n",
       "9   24561.303020        50569.0  \n",
       "10  24561.303020        50569.0  \n",
       "11  18614.403198        30754.0  \n",
       "12  23829.452609        30754.0  \n",
       "13  18703.225000        51646.0  \n",
       "14  19300.202454        30754.0  \n",
       "16  20190.729323        32831.0  \n",
       "17  20333.790123        32708.0  \n",
       "18  21995.431227        30646.0  \n",
       "19  24389.296296        30754.0  \n",
       "20  19111.204754        30754.0  \n",
       "21  13888.813433            NaN  \n",
       "22  16724.535104        50569.0  \n",
       "23  15761.636872        30754.0  \n",
       "24  23982.516358        26969.0  \n",
       "25  26042.914286        57754.0  \n",
       "26  24663.246534        32908.0  \n",
       "27  21655.930452        30754.0  \n",
       "28  22484.937792        23862.0  \n",
       "29  24075.615848        30754.0  \n",
       "30  16180.000000        27831.0  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(label_data,geol_home_df,\n",
    "         left_on=\"twitter_id\",right_on=\"usr\")[[\"twitter_id\",\"Title\",\n",
    "                                               \"income\",\"estimated_sal\"]].drop_duplicates(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___TOO FEW USERS: USE PROFILE DESCRIPTION TO ASSOCIATE JOB____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### APPENDIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_arr(points,proj_to):\n",
    "    inproj = Proj(init='epsg:4326')\n",
    "    outproj = Proj(proj_to)\n",
    "    func = lambda x: transform(inproj,outproj,x[1],x[0])\n",
    "    return np.array(list(map(func, points)))\n",
    "\n",
    "def take_most_frequent(geopandas_usr,min_times=3,min_days=5):\n",
    "    polys_visited=list(geopandas_usr.idINSPIRE)\n",
    "    time_of_visit=[datetime(int(row.year),int(row.month),int(row.fecha),\n",
    "                            int(row.hour),int(row.minu),int(row.sec))\n",
    "                   for it,row in geopandas_usr.iterrows()]\n",
    "    locat_mode=Counter(polys_visited).most_common(1)[0][0]\n",
    "    nb_times=Counter(polys_visited).most_common(1)[0][1]\n",
    "    inter_idx=np.where([x==locat_mode for x in polys_visited])[0].tolist()\n",
    "    time_diff=[time_of_visit[i] for i in inter_idx]\n",
    "    if nb_times>=min_times and (max(time_diff)-min(time_diff)).days>=min_days:\n",
    "        idx_mode=polys_visited.index(locat_mode)\n",
    "        return idx_mode,geopandas_usr.iloc[idx_mode][[\"lat\",\"lon\"]]\n",
    "    else:\n",
    "        return None,None\n",
    "\n",
    "def get_check_in_rate_margin_most_freq(geopandas_usr):\n",
    "    polys_visited=list(geopandas_usr.idINSPIRE)\n",
    "    inter=Counter(polys_visited).most_common(2)\n",
    "    if len(inter)<2:\n",
    "        return None,None,None,None,None\n",
    "    locat_mode,sec_locat_mode=inter\n",
    "    idx_mode,idx_mode_sec=polys_visited.index(locat_mode[0]),polys_visited.index(sec_locat_mode[0])\n",
    "    return (idx_mode,geopandas_usr.iloc[idx_mode][[\"lat\",\"lon\"]],\n",
    "            idx_mode_sec,geopandas_usr.iloc[idx_mode_sec][[\"lat\",\"lon\"]],\n",
    "           ((locat_mode[1]+0.0-sec_locat_mode[1])/(sec_locat_mode[1]+locat_mode[1])))\n",
    "\n",
    "def take_most_frequent_night(geopandas_usr,start=21,stop=6) :\n",
    "    polys_visited=(geopandas_usr.idINSPIRE)\n",
    "    polys_visited_night=polys_visited[(geopandas_usr.hour>=start)|(geopandas_usr.hour<stop)]\n",
    "    if len(polys_visited_night)==0:\n",
    "        return None,None\n",
    "    locat_mode=Counter(polys_visited_night).most_common(1)[0][0]\n",
    "    idx_mode=list(polys_visited).index(locat_mode)\n",
    "    return idx_mode,geopandas_usr.iloc[idx_mode][[\"lat\",\"lon\"]]\n",
    "\n",
    "def get_distance_matrix(geopandas_usr):\n",
    "    x = np.array(geopandas_usr[[\"lat\",\"lon\"]]).astype(float).tolist()\n",
    "    y=proj_arr(x,uk)\n",
    "    ztree = cKDTree(y)\n",
    "    z = ztree.sparse_distance_matrix(ztree,1e6,p=2).todense()\n",
    "    return z\n",
    "\n",
    "def distance_to_home(geopandas_usr,select_home_loc,args):\n",
    "    idx,loc=select_home_loc(geopandas_usr,*args)\n",
    "    if idx is None:\n",
    "        return None,None,None\n",
    "    mat_dist=get_distance_matrix(geopandas_usr)\n",
    "    return mat_dist[idx,:].tolist()[0],list(geopandas_usr.day),list(geopandas_usr.hour)\n",
    "\n",
    "def go_through_home_candidates(dic_gpd,select_home_loc):\n",
    "    dic_exam={}\n",
    "    for usr,gpd in tqdmn(dic_gpd.items()):\n",
    "        idx,loc=select_home_loc(gpd)\n",
    "        if idx is None:\n",
    "            continue\n",
    "        dic_exam.setdefault(usr,gpd.iloc[idx])\n",
    "    return dic_exam\n",
    "\n",
    "def go_through_geol_users(dic_gpd,select_home_loc,args,outlier_lim=6e4):\n",
    "    dic_per_day={k:np.zeros(24) for k in range(7)}\n",
    "    dic_nb_per_day={k:np.zeros(24) for k in range(7)}\n",
    "    dic_exam={}\n",
    "    loss=[]\n",
    "    for usr,gpd in tqdmn(dic_gpd.items()):\n",
    "        dic_exam.setdefault(usr,[])\n",
    "        dists,days,hours=distance_to_home(gpd,select_home_loc,args)\n",
    "        new_dists=np.array(dists)\n",
    "        if dists is None:\n",
    "            continue\n",
    "        loss.append(1-(np.sum([new_dists<outlier_lim])+0.0)/len(dists) )\n",
    "        dists=new_dists[new_dists<outlier_lim]\n",
    "        for dist,day,hour in zip(dists,days,hours):\n",
    "            dic_exam[usr].append(dist)\n",
    "            dic_per_day[day][hour]+=dist\n",
    "            dic_nb_per_day[day][hour]+=1\n",
    "    dic_day={}\n",
    "    for k,v in dic_per_day.items():\n",
    "        dic_day[k]=(v/dic_nb_per_day[k])/100\n",
    "    return dic_day,dic_exam,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
